# -*- coding: utf-8 -*-
"""imdb_sentiment_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1grgoPM3FA5fhA0IWRgZU-JCf0JhbsrAX

# 0Ô∏è‚É£ **Install & Import Necessary Libraries**
"""

# Install necessary package
!pip install gensim

# Import necessary libraries
import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import requests
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, accuracy_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier
import gensim.downloader as api
from joblib import Parallel, delayed
import warnings
warnings.filterwarnings('ignore')  # Suppress warnings for cleaner output
from openai import OpenAI

# Download necessary NLTK data
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('punkt_tab')

# Load the IMDb dataset
file_path = "IMDB Dataset 2.csv"
df = pd.read_csv(file_path)

print("Initial Dataset Shape:", df.shape)
df.head(5)

"""# 1Ô∏è‚É£ **Data Cleaning**"""

print("\nMissing Values Before Cleaning:")
print(df.isnull().sum())

# Drop rows with missing values
df.dropna(inplace=True)

df.head(5)

# Remove duplicates values
df.drop_duplicates(subset=['review'], keep='first', inplace=True)

df.head(5)

# Standardize Text: Convert to Lowercase
df['review'] = df['review'].str.lower()

df.head(5)

# Remove Unnecessary Characters
def clean_text(text):
    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Keep only alphabets
    text = re.sub(r'\s+', ' ', text).strip()  # Remove extra spaces
    return text
df['review'] = df['review'].apply(clean_text)

df['sentiment'] = df['sentiment'].apply(clean_text)

df.head(5)

# Tokenization: Split text into words
df['tokens'] = df['review'].apply(word_tokenize)

df.head()

# Remove Stopwords
stop_words = set(stopwords.words('english'))

df['tokens'] = df['tokens'].apply(lambda words: [word for word in words if word not in stop_words])

df.head(5)

# Ensure sentiment column is string, strip spaces, and convert to lowercase
df['sentiment'] = df['sentiment'].astype(str).str.strip().str.lower()

# Display unique values in 'sentiment' to confirm if any unexpected values exist
unique_sentiments = df['sentiment'].unique()

# Apply mapping to convert sentiment to numerical format
df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})

df.head(5)

# Save Cleaned Dataset
cleaned_file_path = "Cleaned_IMDB_dataset.csv"
df.to_csv(cleaned_file_path, index=False)

df_cleaned = pd.read_csv("Cleaned_IMDB_dataset.csv")
df_cleaned.head(5)

file_path = "Cleaned_IMDB_dataset.csv"
df = pd.read_csv(file_path)

df.head(5)

"""# 2Ô∏è‚É£ Feature Selection

## 1. TF-IDF Vectorization
"""

# Extract text and sentiment labels
X_text = df['review']
y = df['sentiment']

# Split data into training (80%) and testing (20%) sets
X_train, X_test, y_train, y_test = train_test_split(X_text, y, test_size=0.2, random_state=42, stratify=y)

# Convert text data into numerical representation using TF-IDF
tfidf_vectorizer = TfidfVectorizer(max_features=5000) # Limit features to top 5000 for efficiency
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

"""## 2. Word Embeddings"""

# Load Pre-trained GloVe Word Embeddings (50D for efficiency)
glove_model = api.load("glove-wiki-gigaword-50")

# Function to get word embeddings
def get_sentence_embedding(sentence, model, vector_size=50):
    words = sentence.split()
    word_vectors = [model[word] for word in words if word in model]
    return np.mean(word_vectors, axis=0) if word_vectors else np.zeros(vector_size)

# Convert all reviews into sentence embeddings
X_train_glove = np.array([get_sentence_embedding(text, glove_model) for text in X_train])
X_test_glove = np.array([get_sentence_embedding(text, glove_model) for text in X_test])

"""# 3Ô∏è‚É£ **Model Training**

## 1. Train Models on TF-IDF
"""

# Initialize models
log_reg = LogisticRegression()
svm = LinearSVC()
random_forest = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)

# Train Models
log_reg.fit(X_train_tfidf, y_train)
svm.fit(X_train_tfidf, y_train)
random_forest.fit(X_train_tfidf, y_train)

"""## 2. Evaluate Models"""

# Make predictions
y_pred_logreg = log_reg.predict(X_test_tfidf)
y_pred_svm = svm.predict(X_test_tfidf)
y_pred_rf = random_forest.predict(X_test_tfidf)

# Evaluate models
logreg_acc = accuracy_score(y_test, y_pred_logreg)
svm_acc = accuracy_score(y_test, y_pred_svm)
rf_acc = accuracy_score(y_test, y_pred_rf)

# Generate classification reports
logreg_report = classification_report(y_test, y_pred_logreg)
svm_report = classification_report(y_test, y_pred_svm)
rf_report = classification_report(y_test, y_pred_rf)

# Display results
logreg_acc, svm_acc, rf_acc

"""## 3. Re-train LR on Word Embeddings"""

# Train Logistic Regression model on word embeddings
log_reg_glove = LogisticRegression()
log_reg_glove.fit(X_train_glove, y_train)

# Make predictions
y_pred_glove = log_reg_glove.predict(X_test_glove)

# Evaluate model performance
glove_acc = accuracy_score(y_test, y_pred_glove)
glove_report = classification_report(y_test, y_pred_glove)

# Display accuracy and classification report
glove_acc

"""## 4. Comparison Plots

### üìä Comparison of Model Accuracies
"""

logreg_acc = 0.8877684783704749
svm_acc = 0.8828274679842695
rf_acc = 0.8349299183220732

# Create a dictionary to store model names and their accuracies
model_accuracies = {
    "Logistic Regression": logreg_acc,
    "SVM": svm_acc,
    "Random Forest": rf_acc
}

# Convert dictionary to a sorted list for better visualization
models_sorted = sorted(model_accuracies.items(), key=lambda x: x[1], reverse=True)

# Extract model names and accuracies
model_names = [m[0] for m in models_sorted]
accuracies = [m[1] for m in models_sorted]

# Use a cuter pastel-style palette
colors = ['#FEC8D8', '#AFCBFF', '#B6E2D3']  # pastel pink, blue, green

plt.figure(figsize=(7, 4))
sns.barplot(
    x=accuracies,
    y=model_names,
    palette=colors,
    linewidth=1,
    edgecolor='gray',
)

# Add percentage labels to each bar
for i, acc in enumerate(accuracies):
    plt.text(acc + 0.01, i, f"{acc*100:.2f}%", va='center', fontsize=10)

plt.xlabel("Accuracy Score")
plt.ylabel("Models")
plt.title("Comparison of Model Accuracies")
plt.xlim(0, 1)
plt.grid(axis='x', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

"""### üìä TF-IDF + LR v.s. Word Embeddings + LR"""

tfidf_acc = 0.8877684783704749
glove_acc = 0.7572854693959867

# Create a dictionary to store model names and their accuracies
model_accuracies = {
    "TF-IDF\n + Logistic Regression": tfidf_acc,
    "Word Embeddings\n + Logistic Regression": glove_acc
}

# Convert dictionary to a sorted list for better visualization
models_sorted = sorted(model_accuracies.items(), key=lambda x: x[1], reverse=True)

# Extract model names and accuracies
model_names = [m[0] for m in models_sorted]
accuracies = [m[1] for m in models_sorted]

# Plot a bar chart to compare model accuracies
colors = ['#FEC8D8', '#AFCBFF']
plt.figure(figsize=(6.5, 3.5))
sns.barplot(x=accuracies, y=model_names, palette=colors, edgecolor='gray')

# Add percentage labels to each bar
for i, acc in enumerate(accuracies):
    plt.text(acc + 0.01, i, f"{acc*100:.2f}%", va='center', fontsize=10)

plt.xlabel("Accuracy Score")
plt.ylabel("Models")
plt.title("Comparison of Model Accuracies: TF-IDF vs. Word Embeddings")
plt.xlim(0, 1)  # Accuracy range is between 0 and 1
plt.grid(axis='x', linestyle='--', alpha=0.7)

# Display the plot
plt.show()

"""# 4Ô∏è‚É£ **Model Evaluation on Test Set**

## 1. TF-IDF + LR
"""

# Generate Confusion Matrix
cm = confusion_matrix(y_test, y_pred_logreg)

# Plot Confusion Matrix
plt.figure(figsize=(6, 4))

from matplotlib.colors import LinearSegmentedColormap
# Create a custom colormap from cornflower blue to white
custom_cmap = LinearSegmentedColormap.from_list("custom_blue", ["#6495ED", "#EDF6FF"])

sns.heatmap(cm, annot=True, fmt="d", cmap=custom_cmap, xticklabels=["Negative", "Positive"], yticklabels=["Negative", "Positive"])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - Logistic Regression (TF-IDF)")
plt.show()

# Print Classification Report
print("Classification Report:\n", classification_report(y_test, y_pred_logreg))

# Generate ROC Curve
fpr, tpr, _ = roc_curve(y_test, log_reg.predict_proba(X_test_tfidf)[:, 1])
roc_auc = auc(fpr, tpr)

# Plot ROC Curve
plt.figure(figsize=(6, 4))
plt.plot(fpr, tpr, color="#6495ED", lw=2, label=f'ROC Curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color="gray", linestyle="--")  # Random classifier line
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - Logistic Regression (TF-IDF)")
plt.legend(loc="lower right")
plt.show()

"""## 2. Word Embeddings + LR"""

# Generate Confusion Matrix
cm = confusion_matrix(y_test, y_pred_glove)

# Plot Confusion Matrix
plt.figure(figsize=(6, 4))

from matplotlib.colors import LinearSegmentedColormap
# Create a custom colormap from cornflower blue to white
custom_cmap = LinearSegmentedColormap.from_list("custom_blue", ["#6495ED", "#EDF6FF"])

sns.heatmap(cm, annot=True, fmt="d", cmap=custom_cmap, xticklabels=["Negative", "Positive"], yticklabels=["Negative", "Positive"])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - Logistic Regression (Word Embedding)")
plt.show()

# Print Classification Report
print("Classification Report:\n", classification_report(y_test, y_pred_glove))

# Generate ROC Curve
fpr, tpr, _ = roc_curve(y_test, log_reg_glove.predict_proba(X_test_glove)[:, 1])
roc_auc = auc(fpr, tpr)

# Plot ROC Curve
plt.figure(figsize=(6, 4))
plt.plot(fpr, tpr, color="#6495ED", lw=2, label=f'ROC Curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color="gray", linestyle="--")  # Random classifier line
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - Logistic Regression (Word Embedding)")
plt.legend(loc="lower right")
plt.show()

"""# 5Ô∏è‚É£ **Web Scraping for Real-Time Prediction**

## 1. Retrieve Movie Reviews from The Movie Database (TMDB)
"""

# Function to retrieve movie reviews from The Movie Database (TMDB)
def get_movie_reviews(movie_name: str, api_token: str):
    base_url = 'https://api.themoviedb.org/3'

    # Set up authorization headers for the API request
    headers = {
        'Authorization': f'Bearer {api_token}',
        'Accept': 'application/json'
    }

    # Search for the movie to get its unique ID using the provided movie name
    search_response = requests.get(
        f'{base_url}/search/movie',
        headers=headers,
        params={'query': movie_name}
    ).json()

    # Raise an error if no results are found
    if not search_response['results']:
        raise ValueError(f'No movie found for name: {movie_name}')

    # Extract the movie ID of the first matching search result
    movie_id = search_response['results'][0]['id']

    # Initialize pagination variables to iterate through review pages
    page = 1
    all_reviews = []

    # Loop through pages of reviews until all pages have been retrieved
    while True:
        # Request reviews for the specific movie ID and current page
        review_response = requests.get(
            f'{base_url}/movie/{movie_id}/reviews',
            headers=headers,
            params={'page': page}
        ).json()

        # Extract reviews and append them to the list
        reviews = review_response.get('results', [])
        all_reviews.extend([{'Author': r['author'], 'Review': r['content']} for r in reviews])

        # Check if the current page is the last page; break loop if so
        if page >= review_response['total_pages']:
            break

        # Increment page number to retrieve next set of reviews
        page += 1

    # Convert collected reviews to a pandas DataFrame
    review_df = pd.DataFrame(all_reviews)

    # Return a dictionary containing total pages, total results, and the DataFrame of reviews
    return {
        'total_pages': review_response['total_pages'],
        'total_results': review_response['total_results'],
        'reviews_df': review_df
    }

# Example usage of the function with API token and movie name
TMDB_API_TOKEN = 'eyJhbGciOiJIUzI1NiJ9.eyJhdWQiOiJiZWQ4YjVhM2M1ODBlNzJkNjdmYjI2M2IwZjI2MGM4ZCIsIm5iZiI6MTc0MTQwNDAzOC44OTQsInN1YiI6IjY3Y2JiNzg2NDJjNzUyMTI1MmY1OTQ4MyIsInNjb3BlcyI6WyJhcGlfcmVhZCJdLCJ2ZXJzaW9uIjoxfQ.qHPK540tVsfSm9OoL51lkEPRHET9PVXq0s5ej3a3C_8'
movie_reviews = get_movie_reviews('Tenet', TMDB_API_TOKEN)

# Print summary information about reviews
print(movie_reviews['total_pages'])
print(movie_reviews['total_results'])
print(movie_reviews['reviews_df'])

"""## 2. Predict Real-Time Reviews with TF-IDF + LR"""

# Extract text from movie reviews
X_test = movie_reviews['reviews_df']['Review']

# Convert reviews into TF-IDF features
X_test_tfidf = tfidf_vectorizer.transform(X_test)

# Make predictions using Logistic Regression (TF-IDF trained model)
y_pred_tfidf = log_reg.predict(X_test_tfidf)

#Count Percentage of Positive and Negative reviews
positive_count = sum(y_pred_tfidf == 1)
negative_count = sum(y_pred_tfidf == 0)
total_count = len(y_pred_tfidf)
positive_percentage = (positive_count / total_count) * 100
negative_percentage = (negative_count / total_count) * 100
print(f"Percentage of Positive Reviews: {positive_percentage:.2f}%")
print(f"Percentage of Negative Reviews: {negative_percentage:.2f}%")

"""# 6Ô∏è‚É£ **LLM Validation**

## 1. Install Together AI and Import Package
"""

!pip install together
from together import Together

"""## 2. Get LLM-Based Sentiment Labels"""

# Initialize client using API key
client = Together(api_key="662bc1ca6c54e4a0fa6a1b7bcf41482c44316d0c192f93fa363dae2ac9bd4f78")

# Get Sentiment Labels
Y_test_llm = []

for review in X_test[:18]:
    try:
        response = client.chat.completions.create(
            model="meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",  # or any other free Together model
            messages=[
                {
                    "role": "user",
                    "content": (
                        "You are a helpful assistant for sentiment analysis. "
                        "Classify the following movie review as either 1 (positive) or 0 (negative). "
                        "Only return a single digit (0 or 1).\n\nReview:\n" + review
                    )
                }
            ]
        )

        result = response.choices[0].message.content.strip()
        Y_test_llm.append(1 if result == "1" else 0)

    except Exception:
        Y_test_llm.append(0)  # fallback label

"""## 3. Prepare Labels & Predictions"""

# Ensure the length matches with the model predictions
print(len(Y_test_llm), len(y_pred_tfidf))

# Slice other predictions to match
y_pred_glove_short = y_pred_glove[:len(Y_test_llm)]

# Convert LLM results to NumPy array
Y_test_llm = np.asarray(Y_test_llm, dtype=int)

"""## 4. Evaluate TF-IDF Model vs LLM"""

# Evaluate model performance
tfidf_acc = accuracy_score(Y_test_llm, y_pred_tfidf)
tfidf_report = classification_report(Y_test_llm, y_pred_tfidf)

# Display accuracy and classification report
tfidf_acc
print(tfidf_acc)
print(tfidf_report)

# Generate Confusion Matrix
cm = confusion_matrix(Y_test_llm, y_pred_tfidf)

# Plot Confusion Matrix
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt="d", cmap=custom_cmap, xticklabels=["Negative", "Positive"], yticklabels=["Negative", "Positive"])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - Logistic Regression (TF-IDF)")
plt.show()

# Generate ROC Curve
fpr, tpr, _ = roc_curve(Y_test_llm, log_reg.predict_proba(X_test_tfidf)[:, 1])
roc_auc = auc(fpr, tpr)

# Plot ROC Curve
plt.figure(figsize=(6, 4))
plt.plot(fpr, tpr, color="#6495ED", lw=2, label=f'ROC Curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color="gray", linestyle="--")  # Random classifier line
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - Logistic Regression (TF-IDF)")
plt.legend(loc="lower right")
plt.show()

"""## 5. Evaluate Word Embedding Model vs LLM"""

# Evaluate model performance
glove_acc = accuracy_score(Y_test_llm, y_pred_glove_short)
glove_report = classification_report(Y_test_llm, y_pred_glove_short)

# Display accuracy and classification report
glove_acc
print(glove_acc)
print(glove_report)

# Generate Confusion Matrix
cm = confusion_matrix(Y_test_llm, y_pred_glove_short)

# Plot Confusion Matrix
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt="d", cmap=custom_cmap, xticklabels=["Negative", "Positive"], yticklabels=["Negative", "Positive"])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - Logistic Regression (Word Embedding)")
plt.show()

# Generate ROC Curve
fpr, tpr, _ = roc_curve(Y_test_llm, log_reg_glove.predict_proba(X_test_glove[:len(Y_test_llm)])[:, 1])
roc_auc = auc(fpr, tpr)

# Plot ROC Curve
plt.figure(figsize=(6, 4))
plt.plot(fpr, tpr, color="#6495ED", lw=2, label=f'ROC Curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color="gray", linestyle="--")  # Random classifier line
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - Logistic Regression (Word Embedding)")
plt.legend(loc="lower right")
plt.show()

"""# 7Ô∏è‚É£ **User Interface**

### üîó [Try the App Here](https://imdb-sentiment-analysis-movie-reviews.streamlit.app/)  
### üìÇ [View Streamlit Code on GitHub](https://github.com/jessica-chouu/SentimentAnalysis_MovieReviews/blob/main/imdb_streamlit.py)

## Save Trained Vectorizer and Model
"""

import joblib
joblib.dump(tfidf_vectorizer, "tfidf_vectorizer.pkl")
joblib.dump(log_reg, "log_reg_model.pkl")